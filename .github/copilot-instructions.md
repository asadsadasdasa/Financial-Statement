# Copilot Instructions for Financial Statement (CFO Cockpit)

This repository is a *small Streamlit dashboard* used by FP&A/Delivery teams to
visualize **Actuals**, **Forecast** and **Capacity** data.  The core mission of
any code change is to keep the data pipeline robust while preserving the
interactive UI.

## High‑level architecture

1. **`app/`** – the Streamlit application.  `app.py` wires up global filters and
   delegates rendering to three modules in `app/tabs/`.
   - `tab_pl.py` – Profit & Loss charts and tables.
   - `tab_capacity.py` – capacity vs. delivery risk analysis.
   - `tab_simulation.py` – upload and inject custom scenarios.
   - `utils.py` – shared helpers (CSS injection, fiscal month lookup,
     currency formatting).

2. **`app/data_engine.py`** – central ETL and filtering logic.  Almost every
   new feature that touches data (renaming, cleaning, currency conversion,
   fiscal calendar calculation) lives here.  `load_master_data()` reads CSVs
   from the repo root (or `DATA_DIR` env var) using filename heuristics; the
   same function is cached with `@st.cache_data` to speed up repeated runs.
   `filter_and_convert()` applies global filters selected in `app.py` and
   computes an `Amount Final` column using `Currency_Mapping.csv`.

3. **Root CSVs** – the app expects several files in the working directory or
   upward.  Search keywords are hard‑coded (e.g. any CSV containing
   `fagll03`/`actual base` for Actuals or `01_model_revenue` for Forecast).
   The README enumerates them; the code uses `find_file()` in
   `data_engine.py` to locate them.

4. **R&D / adhoc scripts** – `r&d.py` and `Gerar moedas.py` are scratchpads not
   used by the app.  They follow similar helper patterns and can be ignored by
   production agents.

## Data conventions & quirks

- The app must be resilient to **Portuguese/English header variations** from SAP
  exports.  Normalization logic in `load_master_data()` converts accents,
  removes punctuation and runs regex patterns against common tokens.  To
  support a new field, add a `pattern_map` entry or extend the string checks
  around line 220.

- Header detection (`find_header_row`) scans the first 200 lines of an export
  for tokens like `dtlcto`, `conta`, `montante` and returns the number of
  rows to skip.  This avoids manual cleanup of SAP exports.

- Numeric values are cleaned by `clean_financial_number()` which handles
  comma/dot locale differences and returns floats.  Always run this on
  `Amount in local currency` before any arithmetic.

- Fiscal calendar is generated by `apply_fiscal_calendar()`; FY runs from April
  to March.  `meses_fiscais` and `ordem_meses` in `utils.py` hold the mapping.

- The COA mapping file (`COA_Mapping.csv`) is merged to Actuals to infer
  `P&L LVL 5`.  A safety net deduces the P&L bucket from the GL account if the
  mapping is missing (7→Net Sales, 6→Operating Expenses).
- Duplicate columns (eg. same header appearing twice because the SAP
  extract has both "Moeda interna" and "Moeda interna 2" or two
  `P&L LVL 5` fields) historically caused ambiguous-series errors.  The
  loader now **renames extras with `_1`, `_2` suffixes** and warns in the UI
  so data isn’t lost.  After renaming, numeric cleaning and merges operate on
  the first column name and you can inspect the suffixed fields if needed.

- The filter function uses boolean masks; beware of pandas `Series` vs `bool`
  when adding new conditions.  Use `.any()`/`.all()` explicitly to avoid
  ambiguity.

## Developer workflows

1. **Install & run**
   ```powershell
   pip install -r requirements.txt
   streamlit run app/app.py         # from repo root
   # or set DATA_DIR to point at your CSV folder
   ```
   The app will automatically re‑run when files or code change (Streamlit hot
   reload).

2. **Debugging**
   - UI log messages use `st.info()`, `st.warning()`, `st.error()` inside the
     data functions.  When hunting data issues run the app and inspect the
     sidebar output; many steps log column mappings and missing headers.
   - Use the "Upload Data (Simulation)" tab to inject arbitrary DataFrames for
     development without touching disk. Place a print or `st.write()` in a
     tab module to inspect incoming data.

3. **Adding a new data source or field**
   - Update `load_master_data()` to locate and read the file (use
     `find_file()` pattern).
   - Normalize headers with new regex or add entries to `rename_map`.
   - Add validation in `validate_columns()` call to warn when expected
     columns are missing.
   - Update `filter_and_convert()` if the new data participates in global
     filters or currency conversion.

4. **Testing**
   There are currently no automated tests.  Manual smoke tests: run the app,
   load sample CSVs from the root, and verify the tabs render sensible charts.
   Use `r&d.py` to play with DataFrames interactively if needed.

## Project‑specific patterns and conventions

- File lookup is **case‑insensitive** and searches up to three parent
  directories.  Always name templates with the prescribed prefixes so they are
  discovered automatically.

- Header normalization removes accents and punctuation and downcases; you can
  reuse the `_norm()` helper in other scripts.

- When merging on account numbers, both sides are cleaned of trailing `.0`
  and leading zeros to emulate Excel's `VLOOKUP` behaviour.

- Currency mapping files may use `;` separators or comma; `read_csv_smart()`
  attempts to detect the delimiter and encoding (`utf-8`/`latin1`).

- UI filters are generated from the union of all three datasets (Actuals,
  Forecast, Capacity) and include a hard‑coded `'All'` option.  Adding new
  filter dimensions should follow the `get_unq()` pattern in `app.py`.

- The app assumes English column names for calculations but is tolerant of
  Portuguese headers via the normalization step.  Avoid hard‑coding Portuguese
  strings elsewhere in the code.

## External dependencies & integration points

- **Pandas** is the primary data library; the entire logic is data‑frame
  oriented.
- **Streamlit** drives the UI and caching; the code makes few assumptions about
  the runtime and can run in a headless test environment with `st.cache_data`
  warnings.
- Data files are regular CSVs; no database or API is involved.

## Where to look first

- `app/data_engine.py`: data loading, cleaning, and filtering.  Most bugs to do
  with "Erro ao carregar ..." stem from this file.
- `app/app.py`: wiring of filters and session‑state logic for uploads.
- `app/tabs/`: examples of using filtered DataFrames to build charts with
  Plotly and Streamlit.
- `README.md`: reproduces the quick‑start instructions and expected filenames.

---

If anything above is unclear or you spot a missing detail from a recent change,
please ask; I'll refine the guidance.  The goal is that a new Copilot agent can
start contributing without needing to dig through the entire repository.